{"git_branch": "master", "git_hash": "af8a17f8be7c972480f30a02f8bfe6e8b27d933f", "version": "1.0.8", "long_description": "# Python LOPQ module\n\nThis module implements training and testing of LOPQ models along with a variety of other utilities useful for evaluation and data management. It includes a simple implementation of approximate nearest neighbor search with an LOPQ index.\n\n## Usage\n\n```python\nfrom lopq import LOPQModel, LOPQSearcher\n\n# Define a model and fit it to data\nmodel = LOPQModel(V=8, M=4)\nmodel.fit(data)\n\n# Compute the LOPQ codes for a vector\ncode = model.predict(x)\n\n# Create a searcher to index data with the model\nsearcher = LOPQSearcher(model)\nsearcher.add_data(data)\n\n# Retrieve ranked nearest neighbors\nnns = searcher.search(x, quota=100)\n```\n\nA more detailed usage walk-through is found in `scripts/example.py`.\n\n## Training\n\nRefer to the documentation in the `model` submodules and, in particular, the `LOPQModel` class for more usage information.\n\nAvailable parameters for fitting data:\n\n| Name                      | Default | Description                                                               |\n| ------------------------- | ------- | ------------------------------------------------------------------------- |\n| V                         | 8       | The number of clusters per coarse quantizer.                              |\n| M                         | 4       | The total number of fine codes.                                           |\n| kmeans_coarse_iters       | 10      | The number of iterations of k-means for training coarse quantizers.       |\n| kmeans_local_iters        | 20      | The number of iterations of k-means for training subquantizers.           |\n| subquantizer_clusters     | 256     | The number of clusters to train per subquantizer.                         |\n| subquantizer_sample_ratio | 1.0     | The ratio of the data to sample for training subquantizers.               |\n| random_state              | None    | A seed for seeding random operations during training.                     |\n| parameters                | None    | A tuple of trained model parameters to instantiate the model with.        |\n| verbose                   | False   | A boolean indicating whether to produce verbose output.                   |\n\n## Submodules\n\nThere are a handful of submodules, here is a brief description of each.\n\n| Submodule      | Description |\n| -------------- | ----------- |\n| model          | Core training algorithm and the `LOPQModel` class that encapsulates model parameters.\n| search         | An implementation of the multisequence algorithm for retrieval on a multi-index as well as the `LOPQSearcher` class, a simple Python implementation of an LOPQ search index and LOPQ ranking. |\n| eval           | Functions to aid in evaluating and benchmarking trained LOPQ models. |\n| utils          | Miscellaneous utility functions. |\n| lopq_model_pb2 | Protobuf generated module. |\n", "git_origin": "https://github.com/yahoo/lopq.git"}