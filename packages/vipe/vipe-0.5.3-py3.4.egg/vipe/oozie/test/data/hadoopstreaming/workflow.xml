<?xml version="1.0"?>
<!-- Note that documentation placed in comments in this file uses the "markdown" 
	syntax (along with its way of dividing text into sections). -->
<workflow-app xmlns="uri:oozie:workflow:0.3"
	name="test-core_examples_hadoopstreaming_cloner">
	<start to="generate-schema" />

	<action name="generate-schema">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<main-class>eu.dnetlib.iis.core.javamapreduce.hack.AvroSchemaGenerator
			</main-class>
			<arg>eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person
			</arg>
			<capture-output />
		</java>
		<ok to="producer" />
		<error to="fail" />
	</action>

	<action name="producer">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<!-- The data generated by this node is deleted in this section -->
			<prepare>
				<delete path="${nameNode}${workingDir}/producer" />
				<mkdir path="${nameNode}${workingDir}/producer" />
			</prepare>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.core.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.core.examples.java.SampleDataProducer</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Operson=${workingDir}/producer/person</arg>
			<arg>-Odocument=${workingDir}/producer/document</arg>
		</java>
		<ok to="cloner" />
		<error to="fail" />
	</action>
	<action name="cloner">
		<map-reduce>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<prepare>
				<delete path="${nameNode}${workingDir}/cloner" />
				<mkdir path="${nameNode}${workingDir}/cloner" />
			</prepare>
			<streaming>
				<!-- Here, we give the relative path to the script and pass it the parameters 
					of the workflow node. The script is held in a directory having the same name 
					as the workflow node. The parameters should be passed as **named** arguments. 
					This convention of passing them as named arguments makes the code more readable/maintainable. -->
				<mapper>scripts/cloner.py --copies 3</mapper>
				<reducer>scripts/cloner.py --copies 2</reducer>
			</streaming>
			<configuration>
				<!-- # Standard settings for our framework -->
				<property>
					<name>mapred.output.format.class</name>
					<value>com.cloudera.science.avro.streaming.AvroAsJSONOutputFormat
					</value>
				</property>
				<property>
					<name>mapred.input.format.class</name>
					<value>com.cloudera.science.avro.streaming.AvroAsJSONInputFormat
					</value>
				</property>
				<!-- # Custom settings for this workflow node -->
				<property>
					<name>mapred.input.dir</name>
					<value>${workingDir}/producer/person</value>
				</property>
				<property>
					<name>input.schema.literal</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}
					</value>
				</property>
				<property>
					<name>mapred.output.dir</name>
					<value>${workingDir}/cloner/person</value>
				</property>
				<property>
					<name>output.schema.literal</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}
					</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="consumer" />
		<error to="fail" />
	</action>
	<action name="consumer">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.core.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.core.examples.java.PersonTestingConsumer</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Iperson=${workingDir}/cloner/person</arg>
			<arg>-Pexpected_copies=6</arg>
		</java>
		<ok to="end" />
		<error to="fail" />
	</action>
	<kill name="fail">
		<message>Unfortunately, the process failed -- error message:
			[${wf:errorMessage(wf:lastErrorNode())}]
		</message>
	</kill>
	<end name="end" />
</workflow-app>


